


				
				0  Abstract

Environmental perception provides the necessary environmental information for mobile robots to achieve autonomous navigation and intelligent operation [1-2]. Acquisition of unknown environmental information is an important guarantee to realize the technology of  self-positioning [3], visual inspection [4] and path planning for mobile robots.
				
			
Environmental perception involves multi-sensor data fusion technique, which means making measurements using cameras, laser sensors, inertial navigation sensors, etc and performing data fusion on multiple measurements to obtain environmental information.
The joint calibration of multisensor system, which is the establishment of mathematical connection between different senor coordinate systems, is the precondition to realize the fusion of measurement data.
			
In recent years, the joint calibration of camera and laser sensor has become a hot research topic.				
			


In order to achieve joint calibration, calibration objects and calibration features need to be designed to establish the relationship between the two measurement systems. 			
			
Lin Li et al. [7] performed joint calibration based on planar feature matching by extracting the target plane features in LIDAR and camera coordination systems, using planar feature constraints to solve the position and pose parameters and optimizing the initial results to reduce noise. Zhiyu Xiang et al. [8] realized the joint calibration by aiming at the problem of low accuracy when extracting feature points or edge points from LIDAR image, applying distance correspondence principle from the sensor origin to the calibration plane. Q. Zhang et al. [9] proposed a multi-view calibration method based on chessboard calibration plate. According to the different views from camera and laser, geometric constrains are established. The parameters of the camera and laser sensor are obtained initially by minimizing the geometric constraint error and optimized by minimizing the re-projection error. Jingyang Wen et al. [10] assigned the corresponding weights according to the variance values as the initial values of the parameters. The initial values with large errors have smaller weights, and the initial values with smaller errors have larger weights. A more accurate maximum likelihood estimate of the rotation matrix is solved based on this.
			
Most of the referenced work is calibration of monocular camera and 3D laser system. When the visual feature is extracted, it is not possible to determine the spatial position of target point from the same location. More then three positions are needed to calculate the three-dimensional  	 coordinates of the target point. [11]. The proposed methods can not be applied directly to the binocular vision system discussed in this paper. The calibration constraints are classified as distance preserving constraints, collinear and coplanar constraints.			
			
The distance preserving constraints of binocular vision and 3D laser scanning systems can be generated by definition of feature points on the calibration plate. 	
			


To solve the above problems, in this paper, we present a joint calibration method using a black-white chess board with hollow holes.
			
By designing the hollow circular hole on the calibration plate and using its center as the feature point of the 3D laser measurement system, the three-dimensional coordinates of the feature point can be obtained quickly and accurately. The chessboard corner points can be used as the feature points of the visual system, whose three dimensional coordinates can be obtained from binocular vision algorithm. 
By establishing the distance relationship between the 3D coordinates between two groups of feature points, the rotation matrix and translation vector of the two measurement systems can be solved.			
			
The stated method above has high measurement accuracy, less calculation difficulty and improved calibration efficiency and accuracy. 				
			
Through the joint calibration and joint measurement experiments by stereo vision and laser system experiment platform, the reasonable joint calibration parameters and measurement results are obtained, which proves the correctness and feasibility of our method.			
			


1. Joint Measurement System Introduction

1.1  laser measurement system
The laser measurement system used in this article is illustrated in Figure 1. 
	
The 2D laser sensor is mounted on the pitch movement mechanism which constitute a 3D laser measurement system. 
Its scanning plane can perform pitch motion to collect 3D feature information of the environment.
As illustrated in Fig 1, O_l X_l Y_l Z_l is the coordination system for 3D laser measurement, which is not affected by the movement of 2D laser sensor and is a fixed coordinate system. O_BX_BY_BZ_B is the coordinate system of 2D laser sensor, which will change following the pitch motion mechanism and is a moving coordinate system. The origin is located in the optical center. O_B X_B axis is parallel to the axis of rotation of the pitch mechanism. X_B O_B Y_B is the 2D laser scanning plane.
The principle of 2D laser sensor measurement system is shown in Fig. 2, 
where O_B is the optical center of the 2D laser sensor, P is the space point, \alpha the angle value returned by the light beam of the emitting point O_B to the point P in the current scanning plane.	
Suppose \rho is the return distance of laser sensor, the corresponding coordinates of spatial point P in 2D laser coordinate system are:

Equation (1)
				
			
where, x_B, y_B is are 2D coordinates obtained from 2D laser sensor, thus z_B=0. 
			
It is assumed that there is a centain conversion relation between the coordinate system OBXBYBZB of the 2D laser sensor and the coordinate system O l X l Y l Z l of the 3D laser measuring system and can be expressed by the equation (2)

Equation (2)
where <B R l> is the rotation matrix between the two coordinate systems, <l B T> is the translation vector, which can be obtained from the self-calibration experiment of the laser sensor system. Therefor, Eq. (2) realized the conversion from 2D laser sensor to 3D laser measurement system. 


Figure 2 The 2D laser sensor coordinate system


1.2 Binocular stereo vision system

The model of camera imaging can be represented by pinhole model, as shown in Figure 3. 		

Figure 3.  The camera imaging model	
			


According to the pinhole imaging model, the mathematical model of the camera is: 

Equation (3)


Where, (u0 v0) is the coordinate of the main point of the image, (u,v) is the pixel coordinate of the spatial point P(x,y,z), fx and fy are the scale factors of the image coordinate axis. M is the camera projection matrix from 3D space to 2D image plane. This model achieved 3D space to 2D image plane mapping. 


	In the binocular stereo vision system, the world coordinate system O c X c Y c Z c is selected to be coincident with the left camera coordinate system O 1 X 1 Y 1 Z 1. The three-dimensional coordinates of the stereo vision system described herein are both in the world coordinate system O c X c Y c Z c			
			
				
The relationship between the two cameras' geometric positions is shown in Figure 4, where p 1i and p 2i are the projection points of the spatial point P i on the two-dimensional image plane of the left and right cameras respectively.			
	From the two-dimensional pixel coordinates of these two points, according to the parameters calibrated by the stereo camera, the three-dimensional coordinates of the spatial point P i in the stereo vision system (X ci, Y ci, Z ci) can be calculated, where i = 2,..., n		

Figure 4  The geometric model of binocular stereo vision 	
			


1.3  Joint measurement System

When the camera and the laser range finder scan the same scene, the same point in the space is measured, and the three-dimensional coordinates of the point in the binocular stereo vision system and the 3D laser measurement system can be simultaneously obtained. By establishing the relationship between the two three-dimensional coordinates, the rotation matrix R and the translation matrix T of the two sensors in the joint measurement system are obtained.
			
			
Let Pi (where i = 1,2,...,n) be the points in space. If 3D laser measurement system is used, P li (where i = 1,2,...,n) is used to express the 3D coordinates under 3D laser measurement system, which is P li=(X li, Y li, Z li). If binocular stereo vision system is used, P ci (where i = 1,2,...,n) is used to express the 3D coordinates under that system, which is P ci = (X ci, Y ci, Z ci).		
			


When performing joint measurement system data fusion, it is necessary to determine the correspondence between the two sensor coordinate systems.	
			
Measuring two kinds of 3D coordinates of the same point Pi under a single sensor can establish the rotation and translation relations between the two coordinate systems. 			
			
The theoretical relationship between two sensor coordinate systems can be illustrated in Equation (4).


			


Equation (4)

Where, R is the rotation transformation matrix from 3D laser measurement system to binocular stereo vision system.	


				
				In this paper, right-handed coordinate system is used. As shown in Figure 1 and 4,	the coordinate system Ol xl yl zl of the 3D laser measurement system rotates φ around the x axis, θ around the y axis and ψ around the z axis, which will lead to the same point of the coordinate system Oc Xc Yc Zc under binocular stereo vision system.
				
			
The rotation matrix R describing this rotation relationship can be expressed as equation (5).
				
			


Equation (5)


where rij(i,j = 1,2,3) represents the corresponding trigonometric value of the rotation angle.


T is the translation vector of the 3D laser measuring system to the binocular stereo vision system, which can be expressed as:

Equation (6)				
			
2      binocular camera and laser ranging radar joint calibration algorithm

Due to the error of processing and installation, the real value of R and T deviates from the designed value in the physical system, which will lead to a great error in the calculation of the three-dimensional measurement conversion relationship between the two sensors.Therefore, a joint calibration method is designed to correct R and T, thus to minimize dual system joint measurement error.		
			


2.1  	Joint Calibration Method Design

In joint calibration experiments, it is necessary to design a joint measurement object to establish an absolute standard for space measurement in dual-sensing systems.	
			
Therefore, this paper designed hollow chessboard as an experimental calibration plate, shown in figure 5.  	

Figure 5  The schematic diagram of the binocular stereo cameras and laser sensor			
			





















































